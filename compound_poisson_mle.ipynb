{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal is to compute a maximum likelihood estimate (MLE) for a particular compound Poisson distribution. First we setup some definitions. Let\n",
    "\n",
    "\\\\[ N\\sim \\text{Poisson}(\\lambda)  \\text{, and } X_j \\stackrel{i.i.d.}{\\sim} \\text{Geometric}(1-p),\\\\]\n",
    "\n",
    "where the range  of the \\\\(X_j\\\\) starts at 0. The parameters \\\\(\\lambda\\\\) and \\\\(p\\\\) are assumed to be unknown. We can think of each \\\\(X_j\\\\) as the number of successes before the first failure, where success occurs with probability \\\\(p\\\\).\n",
    "\n",
    "We cannot observe \\\\(N\\\\) or the \\\\(X_j\\\\). Instead, we only observe \\\\(n\\\\) i.i.d. copies of \\\\(Y\\\\), where\n",
    "\n",
    "\\\\[ Y = \\sum_{j=1}^N X_j.\\\\]\n",
    "\n",
    "Such a \\\\(Y\\\\) might arise in biology. Perhaps transcription of a gene occurs according to a Poisson process, and once transcription begins, we model the number of mRNA transcripts with a geometric random variable.\n",
    "\n",
    "Let \\\\(\\{Y_i\\}_{i=1}^n\\\\) denote the i.i.d. copies of \\\\(Y\\\\). The goal is to find the maximum likelihood estimates of \\\\(\\lambda\\\\) and \\\\(p\\\\) in a reasonable amount of time without using approximations (other than numerical error). In other words, we want to compute\n",
    "\n",
    "\\\\[ \\underset{\\lambda,p}{\\arg\\max} \\sum_{i=1}^n \\log P(Y=y_i; \\lambda, p)\\\\]\n",
    "\n",
    "subject to the constraints \\\\(\\lambda \\ge 0\\\\) and \\\\(\\epsilon \\le p \\le 1 - \\epsilon\\\\) for some chosen \\\\(\\epsilon \\in (0, 1/2)\\\\).\n",
    "\n",
    "The rest of this document is split into four sections. First, we will write down the probability mass function (p.m.f.) of \\\\(Y\\\\) and its derivatives. Second, we will check the p.m.f. by simulating observations of \\\\(Y\\\\)  and comparing the corresponding histogram to the p.m.f. values. Third, we will check the derivatives via finite differences. Finally, we will use SciPy's optimization library to compute the MLE.\n",
    "\n",
    "It seems that this idea can be extended to a generalized linear model, where the parameters \\\\(\\lambda\\\\) and \\\\(p\\\\) depend on known covariates and unknown parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation of the probability mass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
